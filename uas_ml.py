# -*- coding: utf-8 -*-
"""UAS ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_NL8TubIkuS0w-9ewp3fjeYru-nJIbnj
"""

import pandas as pd
#untuk processing
import re
import nltk

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

"""Dataset"""

from google.colab import drive
drive.mount('/content/drive')

dataset = '/content/drive/MyDrive/PCDML/Amazon_Unlocked_Mobile.csv'
data = pd.read_csv(dataset)
data.head()

# ambil hanya pada brand samsung
data = data.loc[data['Brand Name'] == 'Samsung', 'Brand Name':'Reviews']
# ambil hanya 1000 data
data = data.iloc[0:1000]
# ambil hanya feature Reviews dan Rating
data = data[['Reviews', 'Rating']]
# hapus data mis
data.dropna(inplace=True)

data.head()

"""Preprocessing"""

# melabelkan data menjadi 2 kategori
def pelabelan(rate):
  if rate < 3:
    return 'negatif'
  else:
    return 'positif'

data['Label'] = data['Rating'].apply(pelabelan)
data.head()

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

data['Reviews'][1]

lemma = WordNetLemmatizer()
stemmer = PorterStemmer()
stop_words = set(stopwords.words('english'))

def CleanReview(txt):
  txt = re.sub(r'http\S+', ' ', txt)                                                 # hapus url
  txt = re.sub('[^a-zA-Z]','  ', txt)                                                # hapus teks tidak relevan (karakter selain a-z)
  txt = str(txt).lower()                                                             # jadikan huruf kecil semua
  txt = word_tokenize(txt)                                                           # tokenize
  txt = [item for item in txt if item not in stop_words]                             # hapus stop words ('saya' 'aku' 'kamu' dll)
  txt = [lemma.lemmatize(word=w,pos='v') for w in txt]                               # lemmatization : mengembalikan ke bentuk dasar kata
  txt = [i for i in txt if len(i) > 2]                                               # hapus kata yang kurang dari dua huruf/karakter
  txt = ' '.join(txt)                                                                # penggabungan setiap token menjadi kalimat utuh
  return txt

data['CleanReview'] = data['Reviews'].apply(CleanReview)

data.head()

"""Pervom SVM"""

# split x dan y
x = data['CleanReview']
y = data['Label']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

x_train

# perform count vectorizer
vectorizer = CountVectorizer()
vectorizer.fit(x_train)

# x_train
x_train = vectorizer.transform(x_train)
x_test = vectorizer.transform(x_test)

x_train.toarray()

for c in [0.01, 0.05, 0.25, 0.5, 0.75,  1]:
  svm = LinearSVC(C=c)
  svm.fit(x_train, y_train)
  print('Akurasi untuk c = %s: %s' %(c, accuracy_score(y_test, svm.predict(x_test))))

svm = LinearSVC(C = 0.5)
svm.fit(x_train, y_train)

print('Accuracy score model final: %s ' %accuracy_score(y_test, svm.predict(x_test)))

"""Evaluasi Model"""

y_pred = svm.predict(x_test)
print('Accuracy of SVM classifier on test set: {:.2f}'.format(svm.score(x_test, y_test)))

confusion_matrix = confusion_matrix(y_test, y_pred)
print(confusion_matrix)
print(classification_report(y_test, y_pred))

